{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import warnings\n",
    "import re\n",
    "import ast\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ],
   "id": "87f2f3ac143233c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lift_df = pd.read_csv(\"lifts.csv\", index_col=0)\n",
    "data_for_lift = pd.read_csv(\"data_for_lift.csv\", index_col=0)"
   ],
   "id": "3293295fc3ad5792"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "lift_df",
   "id": "cb0a4ced59ecf038"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# try_df = data_for_lift[data_for_lift[\"Cancer Type\"] == \"Colorectal Carcinoma\"]\n",
    "# try_df = data_for_lift[data_for_lift['Smoke Status'] == \"Unknown\"]\n",
    "# try_df = try_df[try_df['SNP_event'] == \"C>G\"]\n",
    "# try_df = try_df[try_df['Exon_Number'] == \"02/05\"]\n",
    "# try_df = try_df[try_df['Protein_position'] == 12.0]\n",
    "# try_df = try_df[try_df['Codons'] == \"Ggt/Cgt\"]\n",
    "#\n",
    "# set(try_df.index)\n",
    "# try_df"
   ],
   "id": "180954503b0378b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "lift_df.sort_values(by=\"Lift Value\", ascending=False).head(1000)",
   "id": "602d80b7e129097e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df = lift_df.copy()",
   "id": "d034f10a8360d761"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.reset_index(inplace=True)",
   "id": "f780a3a66fb4082f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.sort_values(by=\"Lift Value\", ascending=False)",
   "id": "b6b00c5f7613eab0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Improved function to safely parse tuple-like strings\n",
    "def safe_eval(val):\n",
    "    if pd.isna(val) or val == 'None' or val == 'nan':\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Handle the case where it's already a proper string representation of a tuple\n",
    "        return ast.literal_eval(val)\n",
    "    except (ValueError, SyntaxError):\n",
    "        try:\n",
    "            # Fix missing quotes around words and try again\n",
    "            fixed_val = re.sub(r'(\\b\\w+\\b)(?=\\s*,|\\s*\\))', r\"'\\1'\", val)\n",
    "            fixed_val = re.sub(r'\\(\\s*,', '(', fixed_val)  # Fix empty first item\n",
    "            fixed_val = re.sub(r',\\s*\\)', ')', fixed_val)  # Fix empty last item\n",
    "            return ast.literal_eval(fixed_val)\n",
    "        except (ValueError, SyntaxError):\n",
    "            # If still fails, try to extract content between parentheses\n",
    "            match = re.search(r'\\((.*)\\)', val)\n",
    "            if match:\n",
    "                items = match.group(1).split(',')\n",
    "                # Clean and convert items\n",
    "                processed_items = []\n",
    "                for item in items:\n",
    "                    item = item.strip()\n",
    "                    if item and item != 'None':\n",
    "                        # Try to convert to appropriate type\n",
    "                        try:\n",
    "                            if item.lower() == 'true':\n",
    "                                processed_items.append(True)\n",
    "                            elif item.lower() == 'false':\n",
    "                                processed_items.append(False)\n",
    "                            elif item.replace('.', '', 1).isdigit():\n",
    "                                if '.' in item:\n",
    "                                    processed_items.append(float(item))\n",
    "                                else:\n",
    "                                    processed_items.append(int(item))\n",
    "                            else:\n",
    "                                processed_items.append(item.strip(\"'\\\"\"))\n",
    "                        except:\n",
    "                            processed_items.append(item.strip(\"'\\\"\"))\n",
    "\n",
    "                return tuple(processed_items) if processed_items else None\n",
    "            return None\n",
    "\n",
    "# Apply parsing with tqdm progress bar\n",
    "tqdm.pandas(desc=\"Parsing Features\")\n",
    "df[\"Feature\"] = df[\"Feature\"].progress_apply(lambda x: safe_eval(str(x)) if not pd.isna(x) else None)\n",
    "df[\"Feature Combination\"] = df[\"Feature Combination\"].progress_apply(lambda x: safe_eval(str(x)) if not pd.isna(x) else None)"
   ],
   "id": "694257ff2d0de62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Find all unique features across the dataset\n",
    "all_features = set()\n",
    "for features in df[\"Feature Combination\"].dropna():\n",
    "    if isinstance(features, tuple):  # Make sure it's actually a tuple\n",
    "        all_features.update(features)\n",
    "\n",
    "# Prepare transformed data\n",
    "expanded_data = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Rows\"):\n",
    "    new_row = {\n",
    "        \"Cancer Type\": row[\"Cancer Type\"],\n",
    "        \"Lift Value\": row[\"Lift Value\"]\n",
    "    }\n",
    "\n",
    "    # The bug is here: Check if both are tuples AND verify they're the same length\n",
    "    if isinstance(row[\"Feature Combination\"], tuple) and isinstance(row[\"Feature\"], tuple):\n",
    "        if len(row[\"Feature Combination\"]) == len(row[\"Feature\"]):\n",
    "            new_row.update(dict(zip(row[\"Feature Combination\"], row[\"Feature\"])))\n",
    "        else:\n",
    "            # Handle case where tuples are different lengths\n",
    "            # Only use the matching portions\n",
    "            min_length = min(len(row[\"Feature Combination\"]), len(row[\"Feature\"]))\n",
    "            new_row.update(dict(zip(row[\"Feature Combination\"][:min_length],\n",
    "                                    row[\"Feature\"][:min_length])))\n",
    "\n",
    "    # Assign NaN to missing feature columns\n",
    "    for feature in all_features:\n",
    "        if feature not in new_row:\n",
    "            new_row[feature] = None  # Using None to represent NaN\n",
    "\n",
    "    # Only append rows that have at least one non-null feature value\n",
    "    if any(new_row.get(feature) is not None for feature in all_features):\n",
    "        expanded_data.append(new_row)\n",
    "    else:\n",
    "        # Optionally include even empty rows if that's what you want\n",
    "        expanded_data.append(new_row)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_transformed = pd.DataFrame(expanded_data)"
   ],
   "id": "1091815013ce15b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# d = df_transformed[df_transformed[\"Protein_position\"] == 249]\n",
    "# d[d[\"Cancer Type\"] == \"Intrahepatic Cholangiocarcinoma\"]\n",
    "df_transformed = df_transformed.drop_duplicates()\n",
    "df_transformed = df_transformed[df_transformed[\"Lift Value\"] > 6]\n",
    "df_transformed"
   ],
   "id": "13c0284f8eeda156"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# def find_matching_patients(df_transformed, data_for_lift):\n",
    "#\n",
    "#     # Ensure types match across dataframes (optional, depends on your data)\n",
    "#     df_transformed = df_transformed.copy()\n",
    "#     data_for_lift = data_for_lift.copy()\n",
    "#\n",
    "#     # Pre-index data_for_lift by Cancer Type to reduce row scans\n",
    "#     cancer_type_groups = data_for_lift.groupby('Cancer Type')\n",
    "#\n",
    "#     # Result list to store matched PATIENT_IDs\n",
    "#     matched_ids = []\n",
    "#\n",
    "#     # Iterate through df_transformed\n",
    "#     for idx, row in df_transformed.iterrows():\n",
    "#         cancer_type = row['Cancer Type']\n",
    "#         if cancer_type not in cancer_type_groups.groups:\n",
    "#             matched_ids.append(\"\")\n",
    "#             continue\n",
    "#\n",
    "#         subset = cancer_type_groups.get_group(cancer_type)\n",
    "#\n",
    "#         # Prepare filters: only columns with non-null values, excluding meta-columns\n",
    "#         filter_columns = row.drop(labels=['Cancer Type', 'Lift Value']).dropna()\n",
    "#\n",
    "#         if filter_columns.empty:\n",
    "#             # If no filters, return all patient IDs for this cancer type\n",
    "#             matched = subset['PATIENT_ID'].dropna().unique()\n",
    "#         else:\n",
    "#             # Start with all True boolean mask\n",
    "#             mask = pd.Series(True, index=subset.index)\n",
    "#             for col, val in filter_columns.items():\n",
    "#                 if col in subset.columns:\n",
    "#                     mask &= subset[col] == val\n",
    "#             matched = subset.loc[mask, 'PATIENT_ID'].dropna().unique()\n",
    "#\n",
    "#         # Append as comma-separated string\n",
    "#         matched_ids.append(','.join(map(str, matched)))\n",
    "#\n",
    "#     # Add to DataFrame\n",
    "#     df_transformed['Matched PATIENT_IDs'] = matched_ids\n",
    "#     return df_transformed\n",
    "#\n",
    "#\n",
    "# df_result = find_matching_patients(df_transformed, data_for_lift)\n",
    "\n",
    "def find_matching_patients(df_transformed, data_for_lift):\n",
    "    df_transformed = df_transformed.copy()\n",
    "    data_for_lift = data_for_lift.copy()\n",
    "    cancer_type_groups = data_for_lift.groupby('Cancer Type')\n",
    "    matched_ids = []\n",
    "    matched_counts = []\n",
    "    total_matched_feature_counts = []\n",
    "\n",
    "    for idx, row in df_transformed.iterrows():\n",
    "        cancer_type = row['Cancer Type']\n",
    "        filter_columns = row.drop(labels=['Cancer Type', 'Lift Value']).dropna()\n",
    "\n",
    "        # Find matched patient IDs for this cancer type\n",
    "        if cancer_type not in cancer_type_groups.groups:\n",
    "            matched = []\n",
    "        else:\n",
    "            subset = cancer_type_groups.get_group(cancer_type)\n",
    "            if filter_columns.empty:\n",
    "                matched = subset['PATIENT_ID'].dropna().unique()\n",
    "            else:\n",
    "                mask = pd.Series(True, index=subset.index)\n",
    "                for col, val in filter_columns.items():\n",
    "                    if col in subset.columns:\n",
    "                        mask &= subset[col] == val\n",
    "                matched = subset.loc[mask, 'PATIENT_ID'].dropna().unique()\n",
    "        matched_ids.append(','.join(map(str, matched)))\n",
    "        matched_counts.append(len(matched))\n",
    "\n",
    "        # Find total matched patient IDs with these features across all cancer types\n",
    "        if filter_columns.empty:\n",
    "            total_matched = data_for_lift['PATIENT_ID'].dropna().unique()\n",
    "        else:\n",
    "            mask = pd.Series(True, index=data_for_lift.index)\n",
    "            for col, val in filter_columns.items():\n",
    "                if col in data_for_lift.columns:\n",
    "                    mask &= data_for_lift[col] == val\n",
    "            total_matched = data_for_lift.loc[mask, 'PATIENT_ID'].dropna().unique()\n",
    "        total_matched_feature_counts.append(len(total_matched))\n",
    "\n",
    "    df_transformed['Matched PATIENT_IDs'] = matched_ids\n",
    "    df_transformed['Matched_Count'] = matched_counts\n",
    "    df_transformed['Total_Matched_Feature_Count'] = total_matched_feature_counts\n",
    "    return df_transformed\n",
    "# df_result = find_matching_patients(df_transformed, data_for_lift)\n"
   ],
   "id": "63bbbdc6fabab083"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculate unique patient counts per cancer type\n",
    "cancer_type_patient_counts = data_for_lift.groupby('Cancer Type')['PATIENT_ID'].nunique()\n",
    "# Map to your result DataFrame\n",
    "df_result['Total_Unique_Patients_Cancer_Type'] = df_result['Cancer Type'].map(cancer_type_patient_counts)\n",
    "total_patients = data_for_lift['PATIENT_ID'].nunique()\n",
    "df_result['Total_Patient_IDs'] = total_patients\n",
    "P_A = df_result['Total_Unique_Patients_Cancer_Type'] / total_patients\n",
    "P_B = df_result['Total_Matched_Feature_Count'] / total_patients\n",
    "P_A_B = df_result['Matched_Count'] / total_patients\n",
    "df_result[\"lift_try\"] = P_A_B / (P_A * P_B)\n",
    "df_result"
   ],
   "id": "7e1aba499a964467"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "t = data_for_lift[data_for_lift[\"Smoke Status\"] == \"Unknown\"]\n",
    "t = t[t[\"Chromosome\"] == \"17\"]\n",
    "t = t[t[\"Hugo_Symbol\"] == \"TP53\"]\n",
    "# t = t[t[\"VAR_TYPE_SX\"] == 'Substitution/Indel']\n",
    "# t = t[t[\"Exon_Number\"] == \"21/21\"]\n",
    "# t = t[t[\"Codons\"] == \"cAt/cGt\"]\n",
    "t = t[t[\"SNP_event\"] == \"C>A\"]\n",
    "t = t[t[\"Sex\"] == \"Female\"]\n",
    "# t = t[t[\"Consequence\"] == \"upstream_gene_variant\"]\n",
    "len(list(t[\"PATIENT_ID\"].unique()))"
   ],
   "id": "bf38c9d996fc50d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_result = df_result.sort_values(by=\"lift_try\", ascending=False)\n",
    "unique_ids = df_result['Matched PATIENT_IDs'].drop_duplicates().reset_index(drop=True)\n",
    "id_to_rank = {pid: i+1 for i, pid in unique_ids.items()}\n",
    "df_result['patient_rank'] = df_result['Matched PATIENT_IDs'].map(id_to_rank)\n",
    "df_result"
   ],
   "id": "58ef3d6d0621de6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Group by 'patient_rank' and aggregate unique values as comma-separated strings\n",
    "agg_df = df_result.groupby('patient_rank').agg(\n",
    "    lambda x: ','.join(sorted(map(str, pd.unique(x.dropna()))))\n",
    ").reset_index()\n",
    "\n",
    "agg_df"
   ],
   "id": "335bea7addd87bb8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "agg_df[\"support\"] = 0\n",
    "for idx, row in agg_df.iterrows():\n",
    "    agg_df.at[idx, \"support\"] = len(str(row['Matched PATIENT_IDs']).split(\",\"))"
   ],
   "id": "20a10b90f64b0938"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "agg_df",
   "id": "82a1e830f3d2d16d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_patient_cancer = len(list(data_for_lift[data_for_lift[\"Cancer Type\"] == \"Liver Hepatocellular Carcinoma\"][\"PATIENT_ID\"].unique()))\n",
    "total_patients = len(list(data_for_lift[\"PATIENT_ID\"].unique()))\n",
    "num_patient_cancer / total_patients\n",
    "\n"
   ],
   "id": "dd7d529e0ae588e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "def combine_with_lift_tolerance(df: pd.DataFrame, lift_tolerance: float = 1.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes the result of combine_cancer_data_rows and performs another round of combining\n",
    "    for rows that match all conditions but differ by up to lift_tolerance in lift value.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy to avoid modifying original data\n",
    "    df = df.copy()\n",
    "\n",
    "    # Identify columns to exclude from matching (index-like columns)\n",
    "    id_cols = []\n",
    "    if len(df.columns) > 0:\n",
    "        first_col = df.columns[0]\n",
    "        if (df[first_col].dtype in ['int64', 'float64', 'object'] and\n",
    "            (first_col.lower() in ['id', 'index'] or first_col == df.columns[0])):\n",
    "            id_cols = [first_col]\n",
    "\n",
    "    matching_cols = [col for col in df.columns if col not in id_cols + ['Cancer Type', 'Lift Value']]\n",
    "\n",
    "    # Group by Cancer Type first\n",
    "    cancer_groups = df.groupby('Cancer Type')\n",
    "\n",
    "    combined_rows = []\n",
    "    processed_indices = set()\n",
    "\n",
    "    for cancer_type, cancer_group in cancer_groups:\n",
    "        # Within each cancer type, find lift value groups with tolerance\n",
    "        lift_groups = find_lift_value_groups_with_tolerance(cancer_group, lift_tolerance)\n",
    "\n",
    "        for lift_group_indices in lift_groups:\n",
    "            lift_group = cancer_group.loc[lift_group_indices]\n",
    "\n",
    "            # Skip if any row in this lift group is already processed\n",
    "            if any(idx in processed_indices for idx in lift_group_indices):\n",
    "                continue\n",
    "\n",
    "            # Find combinable row groups within this lift value group\n",
    "            combinable_groups = find_combinable_groups_tolerance(lift_group, matching_cols)\n",
    "\n",
    "            for row_group in combinable_groups:\n",
    "                if any(idx in processed_indices for idx in row_group):\n",
    "                    continue\n",
    "\n",
    "                # Combine the rows in this group\n",
    "                combined_row = combine_row_group_with_lift_avg(lift_group.loc[row_group], id_cols[0] if id_cols else None)\n",
    "                combined_rows.append(combined_row)\n",
    "                processed_indices.update(row_group)\n",
    "\n",
    "    # Convert back to DataFrame\n",
    "    result_df = pd.DataFrame(combined_rows)\n",
    "\n",
    "    # Reorder columns to match original\n",
    "    if not result_df.empty:\n",
    "        result_df = result_df.reindex(columns=df.columns, fill_value=np.nan)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def find_lift_value_groups_with_tolerance(cancer_group: pd.DataFrame, lift_tolerance: float) -> List[List]:\n",
    "    \"\"\"\n",
    "    Group rows by similar lift values within the tolerance.\n",
    "    \"\"\"\n",
    "\n",
    "    rows = cancer_group.index.tolist()\n",
    "    lift_values = cancer_group['Lift Value'].values\n",
    "    n = len(rows)\n",
    "\n",
    "    if n <= 1:\n",
    "        return [rows]\n",
    "\n",
    "    # Use Union-Find to group rows with similar lift values\n",
    "    parent = list(range(n))\n",
    "\n",
    "    def find(x):\n",
    "        if parent[x] != x:\n",
    "            parent[x] = find(parent[x])\n",
    "        return parent[x]\n",
    "\n",
    "    def union(x, y):\n",
    "        px, py = find(x), find(y)\n",
    "        if px != py:\n",
    "            parent[px] = py\n",
    "\n",
    "    # Union rows with lift values within tolerance\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if abs(lift_values[i] - lift_values[j]) <= lift_tolerance:\n",
    "                union(i, j)\n",
    "\n",
    "    # Group rows by their root parent\n",
    "    groups = defaultdict(list)\n",
    "    for i in range(n):\n",
    "        root = find(i)\n",
    "        groups[root].append(rows[i])\n",
    "\n",
    "    return list(groups.values())\n",
    "\n",
    "def find_combinable_groups_tolerance(group: pd.DataFrame, matching_cols: List[str]) -> List[List]:\n",
    "    \"\"\"\n",
    "    Find groups of rows that can be combined based on matching criteria.\n",
    "    Uses Union-Find algorithm to efficiently group compatible rows.\n",
    "    \"\"\"\n",
    "\n",
    "    rows = group.index.tolist()\n",
    "    n = len(rows)\n",
    "\n",
    "    if n <= 1:\n",
    "        return [[row] for row in rows]\n",
    "\n",
    "    # Create compatibility matrix\n",
    "    compatible = [[False] * n for _ in range(n)]\n",
    "\n",
    "    # Check pairwise compatibility\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            row1_idx, row2_idx = rows[i], rows[j]\n",
    "            if can_combine_rows_tolerance(group.loc[[row1_idx, row2_idx]], matching_cols):\n",
    "                compatible[i][j] = compatible[j][i] = True\n",
    "\n",
    "    # Use Union-Find to group compatible rows\n",
    "    parent = list(range(n))\n",
    "\n",
    "    def find(x):\n",
    "        if parent[x] != x:\n",
    "            parent[x] = find(parent[x])\n",
    "        return parent[x]\n",
    "\n",
    "    def union(x, y):\n",
    "        px, py = find(x), find(y)\n",
    "        if px != py:\n",
    "            parent[px] = py\n",
    "\n",
    "    # Union compatible rows\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if compatible[i][j]:\n",
    "                union(i, j)\n",
    "\n",
    "    # Group rows by their root parent\n",
    "    groups = defaultdict(list)\n",
    "    for i in range(n):\n",
    "        root = find(i)\n",
    "        groups[root].append(rows[i])\n",
    "\n",
    "    # Verify each group is fully compatible\n",
    "    final_groups = []\n",
    "    for group_indices in groups.values():\n",
    "        if len(group_indices) == 1:\n",
    "            final_groups.append(group_indices)\n",
    "        else:\n",
    "            # Verify full compatibility within the group\n",
    "            if verify_group_compatibility_tolerance(group.loc[group_indices], matching_cols):\n",
    "                final_groups.append(group_indices)\n",
    "            else:\n",
    "                # If not fully compatible, split into individual rows\n",
    "                final_groups.extend([[idx] for idx in group_indices])\n",
    "\n",
    "    return final_groups\n",
    "\n",
    "def can_combine_rows_tolerance(rows: pd.DataFrame, matching_cols: List[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Check if two or more rows can be combined based on matching criteria.\n",
    "    \"\"\"\n",
    "\n",
    "    matching_count = 0\n",
    "\n",
    "    for col in matching_cols:\n",
    "        col_values = rows[col].dropna()\n",
    "\n",
    "        if len(col_values) == 0:\n",
    "            continue\n",
    "        elif len(col_values) == 1:\n",
    "            # Only one non-null value, no conflict\n",
    "            continue\n",
    "        elif len(set(col_values)) == 1:\n",
    "            # All non-null values are the same\n",
    "            matching_count += 1\n",
    "        else:\n",
    "            # Contradiction: different non-null values\n",
    "            return False\n",
    "\n",
    "    return matching_count >= 3\n",
    "\n",
    "def verify_group_compatibility_tolerance(group: pd.DataFrame, matching_cols: List[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Verify that all rows in a group are mutually compatible.\n",
    "    \"\"\"\n",
    "\n",
    "    matching_count = 0\n",
    "\n",
    "    for col in matching_cols:\n",
    "        col_values = group[col].dropna()\n",
    "\n",
    "        if len(col_values) <= 1:\n",
    "            continue\n",
    "        elif len(set(col_values)) == 1:\n",
    "            matching_count += 1\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    return matching_count >= 3\n",
    "\n",
    "def combine_row_group_with_lift_avg(group: pd.DataFrame, id_col: str = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Combine a group of compatible rows into a single row.\n",
    "    Averages lift values when combining rows with different lift values.\n",
    "    \"\"\"\n",
    "\n",
    "    combined = {}\n",
    "\n",
    "    # Columns that should not be concatenated\n",
    "    single_value_cols = {'Cancer Type'}\n",
    "\n",
    "    for col in group.columns:\n",
    "        non_null_values = group[col].dropna()\n",
    "\n",
    "        if len(non_null_values) == 0:\n",
    "            combined[col] = np.nan\n",
    "        elif col == id_col and id_col is not None:\n",
    "            # For ID column, combine all IDs (handle both string and numeric IDs)\n",
    "            if isinstance(non_null_values.iloc[0], str) and ',' in str(non_null_values.iloc[0]):\n",
    "                # Already combined IDs, merge them\n",
    "                all_ids = []\n",
    "                for val in non_null_values:\n",
    "                    all_ids.extend(str(val).split(','))\n",
    "                combined[col] = ','.join(sorted(set(all_ids)))\n",
    "            else:\n",
    "                combined[col] = ','.join(map(str, sorted(set(non_null_values.tolist()))))\n",
    "        elif col in single_value_cols:\n",
    "            # For Cancer Type, take single value (should be same for all)\n",
    "            combined[col] = non_null_values.iloc[0]\n",
    "        elif col == 'Lift Value':\n",
    "            # For Lift Value, take the average of all values\n",
    "            combined[col] = round(non_null_values.mean(), 2)\n",
    "        else:\n",
    "            # For other columns, take the first non-null value\n",
    "            combined[col] = non_null_values.iloc[0]\n",
    "\n",
    "    return combined\n",
    "\n",
    "# Example usage combining both functions:\n",
    "def full_combine_pipeline(df: pd.DataFrame, lift_tolerance: float = 1.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Complete pipeline: first combine exact matches, then combine with lift tolerance.\n",
    "    \"\"\"\n",
    "\n",
    "    # First pass: combine exact matches (assuming you have the original function)\n",
    "    # combined_df = combine_cancer_data_rows(df)  # Your original function\n",
    "\n",
    "    # Second pass: combine with lift tolerance\n",
    "    final_df = combine_with_lift_tolerance(df, lift_tolerance)\n",
    "\n",
    "    return final_df.sort_values(by=\"Lift Value\", ascending=False)\n",
    "\n",
    "\n",
    "df_after_first_combine = pd.DataFrame(combined_df)\n",
    "final_result = combine_with_lift_tolerance(df_after_first_combine, lift_tolerance=1.0)\n",
    "final_result.sort_values(by=\"Lift Value\", ascending=False)"
   ],
   "id": "fa0f2c24501bf5fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "merged_df.sort_values(by=[\"Lift Value\", \"Cancer Type\"], ascending=[False, True])\n",
    "# df_transformed.sort_values(by=[\"Lift Value\", \"Cancer Type\"], ascending=[False, True])"
   ],
   "id": "2524869e19ccead7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def row_to_sentence(row):\n",
    "    return \" AND \".join(\n",
    "        f\"{col} value is {val}\"\n",
    "        for col, val in row.items()\n",
    "        if pd.notnull(val) and col not in {\"Cancer Type\", \"Lift Value\", \"hypo_factors\"}\n",
    "    )"
   ],
   "id": "f1fe44d38a014bf0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "merged_df['hypo_factors'] = merged_df.apply(row_to_sentence, axis=1)\n",
    "merged_df.rename(columns={'Lift Value': 'support'}, inplace=True)"
   ],
   "id": "1db7d178634c32f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "merged_df",
   "id": "fbf563998dbda47a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "merged_df.to_csv(\"models_hypotheses/LIFT_hypotheses_as_sentences.csv\")",
   "id": "c43634ea55ef7b2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tal = pd.read_csv(\"models_hypotheses/LIFT_hypotheses_as_sentences.csv\")\n",
    "tal.sort_values(by=\"support\", ascending=False)"
   ],
   "id": "7ae3cf95b2f38129"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "t = data_for_lift[data_for_lift[\"Smoke Status\"] == \"Unknown\"]\n",
    "t = t[t[\"Chromosome\"] == \"12\"]\n",
    "t = t[t[\"Hugo_Symbol\"] == \"KRAS\"]\n",
    "t = t[t[\"VAR_TYPE_SX\"] == 'Substitution/Indel']\n",
    "t = t[t[\"Protein_position\"] == 12]\n",
    "t = t[t[\"Exon_Number\"] == \"02/05\"]\n",
    "t = t[t[\"Codons\"] == \"Ggt/Cgt\"]\n",
    "t = t[t[\"SNP_event\"] == \"C>G\"]\n",
    "t = t[t[\"Consequence\"] == \"missense_variant\"]\n",
    "t = t[t[\"Diagnosis Age\"] == \"51-60\"]\n",
    "t = t[t[\"Position\"] == \"25398285.0-25398285.0\"]\n",
    "t"
   ],
   "id": "a4a646ea4bc76fb4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "merged_df.to_csv(\"data_for_lift_merged.csv\")",
   "id": "23e4a5643a6ed0b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "lift_merged = pd.read_csv(\"lifts_merged.csv\", index_col=0)",
   "id": "fdb3296bc465187"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "lift_merged.sort_values(by=\"Lift Value\", ascending=False)",
   "id": "e197dfa20bba6af4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tt = merged_df[merged_df[\"Cancer Type\"] == \"Intrahepatic Cholangiocarcinoma\"]\n",
    "# tt = tt[tt[\"Hugo_Symbolmbol\"] == \"KRAS\"]\n",
    "# tt = tt[tt[\"Smoke Status\"] == \"Unknown\"]\n",
    "# tt = tt[tt[\"Chromosome\"] == \"12\"]\n",
    "# tt = tt[tt[\"VAR_TYPE_SX\"] == 'Substitution/Indel']\n",
    "tt = tt[tt[\"Protein_position\"] == 249]\n",
    "tt"
   ],
   "id": "b0891a3a7021d79d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "('Female', 'Unknown', '12', 'KRAS', 'Substitution/Indel')",
   "id": "1321b9370696ab5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "l = lift_merged[lift_merged['Feature'].astype(str).str.contains('249', na=False)]\n",
    "l = l[l.index == \"Intrahepatic Cholangiocarcinoma\"]\n",
    "l"
   ],
   "id": "c3e8d6136495378b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = lift_merged.copy()\n",
    "df.reset_index(inplace=True)"
   ],
   "id": "7a5049b5cf4e6b2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df[df[\"Lift Value\"] == 27.03]",
   "id": "895f01e994ad4f84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_new = pd.read_csv(\"lifts_second_round.csv\", index_col=0)",
   "id": "324033a6365536a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_new",
   "id": "1499a317bf386f86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "columns = [\n",
    "    'Sex', 'Smoke Status', 'Chromosome', 'Hugo_Symbol', 'SNP_event',\n",
    "    \"Consequence\", 'Exon_Number', \"Diagnosis Age\", \"TMB (nonsynonymous)\",\n",
    "    \"Position\", \"Protein_position\", \"Codons\", \"VAR_TYPE_SX\"\n",
    "]\n",
    "\n",
    "result_frames = []\n",
    "\n",
    "for col_set in combinations(columns, 4):\n",
    "    # Group by the 4 columns and count unique PATIENT_IDs\n",
    "    group = data_for_lift.groupby(list(col_set))['PATIENT_ID'].nunique().reset_index()\n",
    "    # Filter for combinations with at least 50 unique patients\n",
    "    filtered = group[group['PATIENT_ID'] >= 50]\n",
    "    if not filtered.empty:\n",
    "        filtered['Columns'] = ','.join(col_set)\n",
    "        result_frames.append(filtered)\n",
    "\n",
    "# Concatenate all results\n",
    "combinations_df = pd.concat(result_frames, ignore_index=True)\n",
    "combinations_df.head()"
   ],
   "id": "349344e778380dd8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# combinations_df.rename(columns={'PATIENT_ID': 'patient_ids_count'}, inplace=True)\n",
    "combinations_df"
   ],
   "id": "5b9094dd9801c48d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cancer_types = pd.DataFrame({'Cancer Type': data_for_lift['Cancer Type'].unique()})\n",
    "combinations_df = combinations_df.merge(cancer_types, how='cross')"
   ],
   "id": "d2b467ac191b155"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df = find_matching_patients(combinations_df, data_for_lift)\n",
   "id": "5ae1da0eb237fb5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def find_matching_patients(df_transformed, data_for_lift):\n",
    "    df_transformed = df_transformed.copy()\n",
    "    data_for_lift = data_for_lift.copy()\n",
    "    cancer_type_groups = data_for_lift.groupby('Cancer Type')\n",
    "    matched_ids = []\n",
    "    matched_counts = []\n",
    "    total_matched_feature_counts = []\n",
    "\n",
    "    for idx, row in tqdm(df_transformed.iterrows(), total=len(df_transformed), desc=\"Matching patients\"):\n",
    "        cancer_type = row['Cancer Type']\n",
    "        filter_columns = row.drop(labels=['Cancer Type']).dropna()\n",
    "\n",
    "        # Find matched patient IDs for this cancer type\n",
    "        if cancer_type not in cancer_type_groups.groups:\n",
    "            matched = []\n",
    "        else:\n",
    "            subset = cancer_type_groups.get_group(cancer_type)\n",
    "            if filter_columns.empty:\n",
    "                matched = subset['PATIENT_ID'].dropna().unique()\n",
    "            else:\n",
    "                mask = pd.Series(True, index=subset.index)\n",
    "                for col, val in filter_columns.items():\n",
    "                    if col in subset.columns:\n",
    "                        mask &= subset[col] == val\n",
    "                matched = subset.loc[mask, 'PATIENT_ID'].dropna().unique()\n",
    "        matched_ids.append(','.join(map(str, matched)))\n",
    "        matched_counts.append(len(matched))\n",
    "\n",
    "        # Find total matched patient IDs with these features across all cancer types\n",
    "        if filter_columns.empty:\n",
    "            total_matched = data_for_lift['PATIENT_ID'].dropna().unique()\n",
    "        else:\n",
    "            mask = pd.Series(True, index=data_for_lift.index)\n",
    "            for col, val in filter_columns.items():\n",
    "                if col in data_for_lift.columns:\n",
    "                    mask &= data_for_lift[col] == val\n",
    "            total_matched = data_for_lift.loc[mask, 'PATIENT_ID'].dropna().unique()\n",
    "        total_matched_feature_counts.append(len(total_matched))\n",
    "\n",
    "    df_transformed['Matched PATIENT_IDs'] = matched_ids\n",
    "    df_transformed['Matched_Count'] = matched_counts\n",
    "    df_transformed['Total_Matched_Feature_Count'] = total_matched_feature_counts\n",
    "    return df_transformed"
   ],
   "id": "42ff8a708102f5b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df = find_matching_patients(combinations_df, data_for_lift)",
   "id": "f51417c0f3f0e4bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df",
   "id": "e6e4487fd03373ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = df[df[\"Matched_Count\"] > 0]\n",
    "# Calculate unique patient counts per cancer type\n",
    "cancer_type_patient_counts = data_for_lift.groupby('Cancer Type')['PATIENT_ID'].nunique()\n",
    "df['Total_Unique_Patients_Cancer_Type'] = df['Cancer Type'].map(cancer_type_patient_counts)\n",
    "\n",
    "total_patients = data_for_lift['PATIENT_ID'].nunique()\n",
    "\n",
    "P_A = df['Total_Unique_Patients_Cancer_Type'] / total_patients\n",
    "P_B = df['Total_Matched_Feature_Count'] / total_patients\n",
    "P_A_B = df['Matched_Count'] / total_patients\n",
    "\n",
    "df[\"lift_try\"] = P_A_B / (P_A * P_B)\n",
    "df"
   ],
   "id": "252978d5376d5bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = df.sort_values(by=\"lift_try\", ascending=False)\n",
    "unique_ids = df['Matched PATIENT_IDs'].drop_duplicates().reset_index(drop=True)\n",
    "id_to_rank = {pid: i+1 for i, pid in unique_ids.items()}\n",
    "df['patient_rank'] = df['Matched PATIENT_IDs'].map(id_to_rank)\n",
    "df"
   ],
   "id": "fb867e574afb189b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df[df[\"lift_try\"] > 1][\"lift_try\"].describe()",
   "id": "219d850e052539d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dff = df.copy()\n",
    "dff = dff[dff[\"lift_try\"] > 4]\n",
    "dff[\"Matched_Count\"] = dff[\"Matched_Count\"].astype(int)\n",
    "dff = dff[dff[\"Matched_Count\"] >= 15]\n",
    "dff"
   ],
   "id": "45fe1447feae32ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Group by 'patient_rank' and aggregate unique values as comma-separated strings\n",
    "agg_df = dff.groupby('patient_rank').agg(\n",
    "    lambda x: ','.join(sorted(map(str, pd.unique(x.dropna()))))\n",
    ").reset_index()\n",
    "\n",
    "def get_max_from_str(val):\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    vals = [float(x) for x in str(val).split(',') if x.strip()]\n",
    "    return max(vals) if vals else None\n",
    "\n",
    "agg_df['max_lift'] = agg_df['lift_try'].apply(get_max_from_str)\n",
    "agg_df.replace({\"\": None}, inplace=True)\n",
    "agg_df[\"Matched_Count\"] = agg_df[\"Matched_Count\"].astype(int)\n",
    "agg_df.loc[agg_df[\"Smoke Status\"] == \"Unknown\", :] = np.nan\n",
    "agg_df"
   ],
   "id": "891f27eb4701b905"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "#\n",
    "# combine_cols = [\n",
    "#     'Sex', 'Smoke Status', 'Chromosome', 'Hugo_Symbol', 'SNP_event',\n",
    "#     \"Consequence\", 'Exon_Number', \"Diagnosis Age\", \"TMB (nonsynonymous)\",\n",
    "#     \"Position\", \"Protein_position\", \"Codons\", \"VAR_TYPE_SX\"\n",
    "# ]\n",
    "#\n",
    "# def patient_id_overlap(row1, row2):\n",
    "#     set1 = set(str(row1['Matched PATIENT_IDs']).split(','))\n",
    "#     set2 = set(str(row2['Matched PATIENT_IDs']).split(','))\n",
    "#     if not set1 or not set2:\n",
    "#         return False\n",
    "#     intersection = set1 & set2\n",
    "#     union = set1 | set2\n",
    "#     if len(union) == 0:\n",
    "#         return False\n",
    "#     return len(intersection) / len(union) >= 0.8\n",
    "#\n",
    "# def merge_rows(df):\n",
    "#     result_rows = []\n",
    "#     used = set()\n",
    "#     df = df.sort_values('max_lift', ascending=False).reset_index(drop=True)\n",
    "#\n",
    "#     for cancer_type, group in tqdm(df.groupby('Cancer Type'), desc=\"Merging by Cancer Type\"):\n",
    "#         group = group.reset_index()  # preserve original index\n",
    "#         n = len(group)\n",
    "#\n",
    "#         for i in range(n):\n",
    "#             idx_i = group.loc[i, 'index']\n",
    "#             if idx_i in used:\n",
    "#                 continue\n",
    "#\n",
    "#             base_row = group.loc[i]\n",
    "#             merge_candidates = [i]\n",
    "#\n",
    "#             for j in range(n):\n",
    "#                 if i == j:\n",
    "#                     continue\n",
    "#\n",
    "#                 idx_j = group.loc[j, 'index']\n",
    "#                 if idx_j in used:\n",
    "#                     continue\n",
    "#\n",
    "#                 comp_row = group.loc[j]\n",
    "#\n",
    "#                 if not patient_id_overlap(base_row, comp_row):\n",
    "#                     continue\n",
    "#                 matches = 0\n",
    "#                 consistent = True\n",
    "#                 for col in combine_cols:\n",
    "#                     val1 = base_row[col]\n",
    "#                     val2 = comp_row[col]\n",
    "#                     if pd.notna(val1) and pd.notna(val2):\n",
    "#                         if val1 != val2:\n",
    "#                             consistent = False\n",
    "#                             break\n",
    "#                         matches += 1\n",
    "#\n",
    "#                 if consistent and matches >= 3 and patient_id_overlap(base_row, comp_row):\n",
    "#                     merge_candidates.append(j)\n",
    "#\n",
    "#             merge_indices = [group.loc[k, 'index'] for k in merge_candidates]\n",
    "#             if len(merge_candidates) > 1:\n",
    "#                 subset = group.loc[merge_candidates]\n",
    "#                 merged_row = base_row.copy()\n",
    "#                 for col in subset.columns:\n",
    "#                     vals = subset[col].dropna().astype(str).unique()\n",
    "#                     if col in [\"Columns\", \"Matched PATIENT_IDs\"]:\n",
    "#                         unique_vals = set()\n",
    "#                         for v in vals:\n",
    "#                             unique_vals.update(map(str.strip, v.split(',')))\n",
    "#                         merged_row[col] = ','.join(sorted(unique_vals))\n",
    "#                     else:\n",
    "#                         merged_row[col] = ','.join(sorted(set(vals)))\n",
    "#                 patient_ranks = subset['patient_rank'].dropna().astype(int)\n",
    "#                 merged_row['patient_rank_agg'] = ','.join(sorted(map(str, set(patient_ranks))))\n",
    "#                 merged_row['patient_rank_min'] = patient_ranks.min() if not patient_ranks.empty else None\n",
    "#                 result_rows.append(merged_row.drop(labels='index'))\n",
    "#                 used.update(merge_indices)\n",
    "#             else:\n",
    "#                 base_row = base_row.copy()\n",
    "#                 if pd.notna(base_row['patient_rank']):\n",
    "#                     base_row['patient_rank_agg'] = str(int(base_row['patient_rank']))\n",
    "#                     base_row['patient_rank_min'] = int(base_row['patient_rank'])\n",
    "#                 else:\n",
    "#                     base_row['patient_rank_agg'] = ''\n",
    "#                     base_row['patient_rank_max'] = None\n",
    "#                 result_rows.append(base_row.drop(labels='index'))\n",
    "#                 used.add(idx_i)\n",
    "#\n",
    "#     return pd.DataFrame(result_rows)\n",
    "#\n",
    "# merged_df = merge_rows(agg_df)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "combine_cols = [\n",
    "    'Sex', 'Smoke Status', 'Chromosome', 'Hugo_Symbol', 'SNP_event',\n",
    "    \"Consequence\", 'Exon_Number', \"Diagnosis Age\", \"TMB (nonsynonymous)\",\n",
    "    \"Position\", \"Protein_position\", \"Codons\", \"VAR_TYPE_SX\"\n",
    "]\n",
    "\n",
    "def patient_id_overlap(set1, set2):\n",
    "    if not set1 or not set2:\n",
    "        return False\n",
    "    intersection = set1 & set2\n",
    "    union = set1 | set2\n",
    "    if len(union) == 0:\n",
    "        return False\n",
    "    return len(intersection) / len(union) >= 0.8\n",
    "\n",
    "def merge_rows(df):\n",
    "    result_rows = []\n",
    "    used = set()\n",
    "    df = df.sort_values('max_lift', ascending=False).reset_index(drop=True)\n",
    "    # Precompute patient ID sets for all rows\n",
    "    df['_pid_set'] = df['Matched PATIENT_IDs'].apply(lambda x: set(str(x).split(',')) if pd.notna(x) else set())\n",
    "\n",
    "    for cancer_type, group in tqdm(df.groupby('Cancer Type', sort=False), desc=\"Merging by Cancer Type\"):\n",
    "        group = group.reset_index()  # preserve original index\n",
    "        n = len(group)\n",
    "        # Precompute patient ID sets for group\n",
    "        pid_sets = group['_pid_set'].tolist()\n",
    "        for i in range(n):\n",
    "            idx_i = group.loc[i, 'index']\n",
    "            if idx_i in used:\n",
    "                continue\n",
    "            base_row = group.loc[i]\n",
    "            base_pid_set = pid_sets[i]\n",
    "            merge_candidates = [i]\n",
    "            for j in range(i + 1, n):  # Only check j > i to avoid redundant checks\n",
    "                idx_j = group.loc[j, 'index']\n",
    "                if idx_j in used:\n",
    "                    continue\n",
    "                comp_row = group.loc[j]\n",
    "                comp_pid_set = pid_sets[j]\n",
    "                if not patient_id_overlap(base_pid_set, comp_pid_set):\n",
    "                    continue\n",
    "                matches = 0\n",
    "                consistent = True\n",
    "                for col in combine_cols:\n",
    "                    val1 = base_row[col]\n",
    "                    val2 = comp_row[col]\n",
    "                    if pd.notna(val1) and pd.notna(val2):\n",
    "                        if val1 != val2:\n",
    "                            consistent = False\n",
    "                            break\n",
    "                        matches += 1\n",
    "                if consistent and matches >= 3:\n",
    "                    merge_candidates.append(j)\n",
    "            merge_indices = [group.loc[k, 'index'] for k in merge_candidates]\n",
    "            if len(merge_candidates) > 1:\n",
    "                subset = group.loc[merge_candidates]\n",
    "                merged_row = base_row.copy()\n",
    "                for col in subset.columns:\n",
    "                    vals = subset[col].dropna().astype(str).unique()\n",
    "                    if col in [\"Columns\", \"Matched PATIENT_IDs\"]:\n",
    "                        unique_vals = set()\n",
    "                        for v in vals:\n",
    "                            unique_vals.update(map(str.strip, v.split(',')))\n",
    "                        merged_row[col] = ','.join(sorted(unique_vals))\n",
    "                    else:\n",
    "                        merged_row[col] = ','.join(sorted(set(vals)))\n",
    "                patient_ranks = subset['patient_rank'].dropna().astype(int)\n",
    "                merged_row['patient_rank_agg'] = ','.join(sorted(map(str, set(patient_ranks))))\n",
    "                merged_row['patient_rank_min'] = patient_ranks.min() if not patient_ranks.empty else None\n",
    "                result_rows.append(merged_row.drop(labels=['index', '_pid_set']))\n",
    "                used.update(merge_indices)\n",
    "            else:\n",
    "                base_row = base_row.copy()\n",
    "                if pd.notna(base_row['patient_rank']):\n",
    "                    base_row['patient_rank_agg'] = str(int(base_row['patient_rank']))\n",
    "                    base_row['patient_rank_min'] = int(base_row['patient_rank'])\n",
    "                else:\n",
    "                    base_row['patient_rank_agg'] = ''\n",
    "                    base_row['patient_rank_max'] = None\n",
    "                result_rows.append(base_row.drop(labels=['index', '_pid_set']))\n",
    "                used.add(idx_i)\n",
    "    return pd.DataFrame(result_rows)\n",
    "\n",
    "merged_df = merge_rows(agg_df)"
   ],
   "id": "c74b2dbff555dc38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "merged_df.replace(\"\", np.nan, inplace=True)\n",
    "merged_df['max_lift'] = merged_df['lift_try'].apply(get_max_from_str)"
   ],
   "id": "b256e495fab564f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "merged_df",
   "id": "bcbf7e56e7712be9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "dff[dff[\"patient_rank\"].isin([1,2,3,4])]",
   "id": "1b8c311dd667e94d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "d = data_for_lift[data_for_lift[\"Smoke Status\"] == \"Unknown\"]\n",
    "# d = d[d[\"Chromosome\"] == \"13\"]\n",
    "d = d[d[\"Consequence\"] == \"missense_variant\"]\n",
    "# d = d[d[\"Diagnosis Age\"] == '31-40']\n",
    "d = d[d[\"Codons\"] == \"tGc/tAc\"]\n",
    "d = d[d[\"SNP_event\"] == \"G>A\"]\n",
    "d[\"Cancer Type\"].value_counts()"
   ],
   "id": "4e5d81b4ffc62101"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def row_to_sentence(row):\n",
    "    return \" AND \".join(\n",
    "        f\"{col.replace(\"_\", \" \")} is {val}\"\n",
    "        for col, val in row.items()\n",
    "        if pd.notnull(val) and col not in {\"patient_rank\", \"patient_ids_count\", \"Columns\", \"Cancer Type\", \"lift_try\", \"max_lift\", \"hypo_factors\", 'patient_rank_agg', 'patient_rank_min', 'Matched PATIENT_IDs', 'Matched_Count', 'Total_Matched_Feature_Count',\n",
    "       'Total_Unique_Patients_Cancer_Type'}\n",
    "    )"
   ],
   "id": "f79efd189288e6c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "merged_df.columns",
   "id": "861bbbb4165d2804"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "merged_df['hypo_factors'] = merged_df.apply(row_to_sentence, axis=1)\n",
    "# merged_df.rename(columns={'Lift Value': 'support'}, inplace=True)"
   ],
   "id": "a55a6712bed165b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "merged_df.head()",
   "id": "8a30152cfdc3640d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "final_lift = merged_df.drop(labels=[\"patient_rank\", \"patient_ids_count\", \"Columns\", \"lift_try\", 'patient_rank_agg', 'patient_rank_min', 'Matched PATIENT_IDs', 'Total_Matched_Feature_Count',\n",
    "       'Total_Unique_Patients_Cancer_Type'], axis=1)"
   ],
   "id": "4f821ea3bf3feddf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "final_lift.rename(columns={'Matched_Count':'support'}, inplace=True)",
   "id": "60123eb2e43cee58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "final_lift.to_csv(\"models_hypotheses/LIFT_hypotheses_as_sentences.csv\", index=False)",
   "id": "77b03b7c845ed392"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "dc62410c36381d58"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
