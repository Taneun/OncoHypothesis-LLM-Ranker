{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import ttest_ind\n",
    "import itertools\n",
    "from scipy.stats import f_oneway\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from sympy.abc import alpha"
   ],
   "id": "b964a0868c9bde11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# def get_latest_csv(directory):\n",
    "#     list_of_files = glob.glob(os.path.join(directory, '*.csv'))\n",
    "#     if not list_of_files:\n",
    "#         raise FileNotFoundError(\"No CSV files found in the directory.\")\n",
    "#     latest_file = max(list_of_files, key=os.path.getmtime)\n",
    "#     print(f\"Using file: {latest_file}\")\n",
    "#     return latest_file"
   ],
   "id": "f5d2d6bd4db72f2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# latest_csv_path = get_latest_csv(\"llm_results\")\n",
    "# df = pd.read_csv(latest_csv_path)"
   ],
   "id": "a208fcf6ba1603a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_all_csv(directory):\n",
    "    # Exclude files containing 'gpt4o' in their name\n",
    "    list_of_files = [\n",
    "        f for f in glob.glob(os.path.join(directory, '*.csv'))\n",
    "        if 'gpt4o' not in os.path.basename(f) and (os.path.basename(f).startswith(\"evaluations_lift_\"))]\n",
    "    if not list_of_files:\n",
    "        raise FileNotFoundError(\"No CSV files found in the directory (excluding gpt4o).\")\n",
    "    return list_of_files\n",
    "\n",
    "def get_avg_result_accross_csv(list_of_files):\n",
    "    df_list = []\n",
    "    for file in list_of_files:\n",
    "        # Extract timestamp using regex\n",
    "        df_temp = pd.read_csv(file)\n",
    "        df_temp['timestamp'] = file.split(\"/\")[-1].split(\"_\")[2]\n",
    "        df_list.append(df_temp)\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # Compute averages per hypothesis_id and model\n",
    "    avg_df = combined_df.groupby(['hypothesis_id', 'model'])[['novelty', 'plausibility']].mean().reset_index()\n",
    "    avg_df.rename(columns={'novelty': 'average_novelty', 'plausibility': 'average_plausibility'}, inplace=True)\n",
    "\n",
    "    # Calculate std for novelty and plausibility per hypothesis_id and model\n",
    "    std_df = combined_df.groupby(['hypothesis_id', 'model'])[['novelty', 'plausibility']].std().reset_index()\n",
    "    std_df.rename(columns={'novelty': 'std_novelty', 'plausibility': 'std_plausibility'}, inplace=True)\n",
    "\n",
    "    # Merge only the new average columns to keep all original columns\n",
    "    combined_df = combined_df.merge(\n",
    "        avg_df[['hypothesis_id', 'model', 'average_novelty', 'average_plausibility']],\n",
    "        on=['hypothesis_id', 'model'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Merge std values back into the main DataFrame\n",
    "    combined_df = combined_df.merge(\n",
    "        std_df,\n",
    "        on=['hypothesis_id', 'model'],\n",
    "        how='left'\n",
    "    )\n",
    "    return combined_df\n",
    "\n",
    "# Usage\n",
    "list_of_files = get_all_csv(\"llm_results\")\n",
    "df = get_avg_result_accross_csv(list_of_files)\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df.sort_values([\"hypothesis_id\", \"timestamp\", \"model\"], inplace=True)\n",
    "# df = df[[\"hypothesis_id\", \"model\", \"average_novelty\", \"average_plausibility\",]]\n",
    "df.drop_duplicates(inplace=True)\n",
    "df"
   ],
   "id": "bb98b076c79827d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_name_mapping = {\n",
    "        'openai:o4-mini': 'OpenAI o4-mini',\n",
    "        'openai:o3-mini': 'OpenAI o3-mini',\n",
    "        'anthropic:claude-3-7-sonnet-latest': 'Claude 3.7 Sonnet'\n",
    "    }\n",
    "df['model'] = df['model'].map(model_name_mapping)"
   ],
   "id": "865160b8b22c4e49"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Use the same number of rows as your original df\n",
    "random_df = df[['hypothesis_id']].copy()\n",
    "random_df['model'] = 'Random'\n",
    "random_df['novelty'] = np.random.uniform(0, 10, size=len(random_df))\n",
    "random_df['plausibility'] = np.random.uniform(0, 10, size=len(random_df))\n",
    "\n",
    "# Concatenate with original df\n",
    "df_with_random = pd.concat([df[['hypothesis_id', 'model', 'novelty', 'plausibility']], random_df], ignore_index=True)"
   ],
   "id": "87aea259bf49f275"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "stds = (\n",
    "    df_with_random.groupby(['hypothesis_id', 'model'])[['novelty', 'plausibility']]\n",
    "    .std()\n",
    "    .reset_index()\n",
    "    .melt(id_vars=['hypothesis_id', 'model'], value_vars=['novelty', 'plausibility'],\n",
    "          var_name='score_type', value_name='std')\n",
    ")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=stds, x='model', y='std', hue='score_type', palette='pastel', showfliers=False)\n",
    "# plt.title(\"Model Consistency Across Runs (Lower STD = More Consistent)\")\n",
    "plt.ylabel(\"Standard Deviation Across Runs\", fontsize=14)\n",
    "plt.xlabel(\"Model\", fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylim(0, 10)\n",
    "plt.legend(fontsize=14, title_fontsize='15')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "6c7cdba0593e2032"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "avg_std = df.groupby('model')[['std_novelty', 'std_plausibility']].mean().reset_index()\n",
    "# avg_std['avg_std'] = avg_std[['std_novelty', 'std_plausibility']].mean(axis=1)\n",
    "\n",
    "avg_std[\"std_novelty\"] = avg_std[\"std_novelty\"].round(2)\n",
    "avg_std[\"std_plausibility\"] = avg_std[\"std_plausibility\"].round(2)\n",
    "print(avg_std)"
   ],
   "id": "bd37aafa49b73a0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "combine_hyp = pd.read_csv('models_hypotheses/combined_hypotheses_lift_only.csv')\n",
    "combine_hyp.rename(columns={'hypo_id': 'hypothesis_id'}, inplace=True)"
   ],
   "id": "a2e961b23c8b7b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Make sure both columns are the same type (e.g., convert both to string or both to int)\n",
    "df['hypothesis_id'] = df['hypothesis_id'].astype(str)\n",
    "combine_hyp['hypothesis_id'] = combine_hyp['hypothesis_id'].astype(str)\n",
    "\n",
    "# Now you can safely join\n",
    "df = df.join(combine_hyp.set_index('hypothesis_id'), on='hypothesis_id')\n"
   ],
   "id": "53e92dfd17611a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create dummies from Consequence\n",
    "dummy_vars = df['Consequence'].str.split(',').explode().str.get_dummies().groupby(level=0).sum()\n",
    "\n",
    "for col in dummy_vars.columns:\n",
    "    if col in df.columns:\n",
    "        # Update only where the new dummy is 1\n",
    "        df.loc[dummy_vars[col] == 1, col] = 1\n",
    "    else:\n",
    "        df[col] = dummy_vars[col]\n",
    "\n",
    "df.drop('Consequence', axis=1, inplace=True)"
   ],
   "id": "70ab6e1ce0f6dea1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df.sort_values(by=['novelty', 'plausibility'], ascending=[False, False], inplace=True)\n",
    "df.drop(columns=['novelty', 'plausibility', \"timestamp\"], inplace=True)\n",
    "df.rename(columns={'average_novelty': 'novelty', 'average_plausibility': 'plausibility'}, inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df"
   ],
   "id": "bc57e97c0dbb0eb0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "novelty_matrix = df.pivot(index=['hypothesis_id', 'hypo_factors', 'cancer_type'], columns='model', values='novelty')\n",
    "plausibility_matrix = df.pivot(index='hypothesis_id', columns='model', values='plausibility')"
   ],
   "id": "e5b61bed46c7adac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "novelty_matrix",
   "id": "5c04ebaae1485a94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df.groupby(['hypothesis_id']).agg({\n",
    "    'novelty': ['mean', 'std'],\n",
    "    'plausibility': ['mean', 'std']\n",
    "}).reset_index()"
   ],
   "id": "5af1e4037f74b55d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_pivot = df.pivot(index='hypothesis_id', columns='model', values='novelty')\n",
    "\n",
    "# Compute standard deviation per sample (variation between models)\n",
    "df_pivot['std'] = df_pivot.std(axis=1)\n",
    "df_pivot['range'] = df_pivot.max(axis=1) - df_pivot.min(axis=1)\n",
    "\n",
    "# Compute pairwise MAE and RMSE between models\n",
    "def compare_models(col1, col2):\n",
    "    mae = mean_absolute_error(df_pivot[col1], df_pivot[col2])\n",
    "    rmse = np.sqrt(mean_squared_error(df_pivot[col1], df_pivot[col2]))\n",
    "    return mae, rmse\n",
    "\n",
    "mae_ab, rmse_ab = compare_models('OpenAI o4-mini', 'Claude 3.7 Sonnet')\n",
    "mae_ac, rmse_ac = compare_models('OpenAI o4-mini', 'OpenAI o3-mini')\n",
    "mae_bc, rmse_bc = compare_models('Claude 3.7 Sonnet', 'OpenAI o3-mini')\n",
    "\n",
    "# Output results\n",
    "\n",
    "print(\"Mean Absolute Errors - Novelty:\")\n",
    "print(f\"Model o4-mini vs anthropic: {mae_ab:.2f}\")\n",
    "print(f\"Model o4-mini vs o3-mini: {mae_ac:.2f}\")\n",
    "print(f\"Model anthropic vs o3-mini: {mae_bc:.2f}\")\n",
    "\n",
    "print(\"\\nRMSE - Novelty:\")\n",
    "print(f\"Model o4-mini vs anthropic: {rmse_ab:.2f}\")\n",
    "print(f\"Model o4-mini vs o3-mini: {rmse_ac:.2f}\")\n",
    "print(f\"Model anthropic vs o3-mini: {rmse_bc:.2f}\")\n",
    "\n",
    "df_pivot = df.pivot(index='hypothesis_id', columns='model', values='plausibility')\n",
    "\n",
    "# Compute standard deviation per sample (variation between models)\n",
    "df_pivot['std'] = df_pivot.std(axis=1)\n",
    "df_pivot['range'] = df_pivot.max(axis=1) - df_pivot.min(axis=1)\n",
    "\n",
    "# Compute pairwise MAE and RMSE between models\n",
    "def compare_models(col1, col2):\n",
    "    mae = mean_absolute_error(df_pivot[col1], df_pivot[col2])\n",
    "    rmse = np.sqrt(mean_squared_error(df_pivot[col1], df_pivot[col2]))\n",
    "    return mae, rmse\n",
    "\n",
    "mae_ab, rmse_ab = compare_models('OpenAI o4-mini', 'Claude 3.7 Sonnet')\n",
    "mae_ac, rmse_ac = compare_models('OpenAI o4-mini', 'OpenAI o3-mini')\n",
    "mae_bc, rmse_bc = compare_models('Claude 3.7 Sonnet', 'OpenAI o3-mini')\n",
    "\n",
    "# Output results\n",
    "\n",
    "print(\"\\nMean Absolute Errors - Plausibility:\")\n",
    "print(f\"Model o4-mini vs anthropic: {mae_ab:.2f}\")\n",
    "print(f\"Model o4-mini vs o3-mini: {mae_ac:.2f}\")\n",
    "print(f\"Model anthropic vs o3-mini: {mae_bc:.2f}\")\n",
    "\n",
    "print(\"\\nRMSE - Plausibility:\")\n",
    "print(f\"Model o4-mini vs anthropic: {rmse_ab:.2f}\")\n",
    "print(f\"Model o4-mini vs o3-mini: {rmse_ac:.2f}\")\n",
    "print(f\"Model anthropic vs o3-mini: {rmse_bc:.2f}\")"
   ],
   "id": "a55a3a775d1b7979"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "grouped = df.groupby(\"model\")[\"novelty\"].apply(list)\n",
    "\n",
    "f_stat, p_val = f_oneway(*grouped)\n",
    "print(f\"F-statistic: {f_stat:.4f}, p-value: {p_val:.4e}\")"
   ],
   "id": "868229665635a20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "grouped = df.groupby(\"model\")[\"plausibility\"].apply(list)\n",
    "\n",
    "f_stat, p_val = f_oneway(*grouped)\n",
    "print(f\"F-statistic: {f_stat:.4f}, p-value: {p_val:.4e}\")"
   ],
   "id": "72e680e48e8dcbb2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# computing Z score\n",
    "df[['novelty_norm', 'plausibility_norm']] = df.groupby('model')[['novelty', 'plausibility']].transform(\n",
    "    lambda x: (x - x.mean()) / x.std()\n",
    ")"
   ],
   "id": "7fd3d96eb94da456"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#distribution before normalization\n",
    "# df = merged_df.copy()\n",
    "df['model'] = df['model'].astype(str)\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Novelty distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plot1 = sns.kdeplot(data=df,x=\"novelty\", hue=\"model\", common_norm=False, fill=True, linewidth=2, legend=False)\n",
    "plt.xlabel(\"Novelty Score\", fontsize=22)\n",
    "plt.ylabel(\"Density\", fontsize=22)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plausibility distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plot2 = sns.kdeplot(data=df,x=\"plausibility\", hue=\"model\", common_norm=False, fill=True, linewidth=2, legend=False)\n",
    "plt.xlabel(\"Plausibility Score\", fontsize=22)\n",
    "plt.ylabel(\"Density\", fontsize=22)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "c00aef38d305245b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Ensure 'model' is treated as a categorical variable\n",
    "# df = merged_df.copy()\n",
    "df['model'] = df['model'].astype(str)\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Novelty distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plot1 = sns.kdeplot(data=df,x=\"novelty_norm\", hue=\"model\", common_norm=False, fill=True, linewidth=2, legend=False)\n",
    "plt.xlabel(\"Novelty Score\", fontsize=22)\n",
    "plt.ylabel(\"Density\", fontsize=22)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plausibility distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plot2 = sns.kdeplot(data=df,x=\"plausibility_norm\", hue=\"model\", common_norm=False, fill=True, linewidth=2, legend=False)\n",
    "plt.xlabel(\"Plausibility Score\" ,fontsize=22)\n",
    "plt.ylabel(\"Density\", fontsize=22)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "5c09c095ca06d3d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculate the 90th percentile thresholds\n",
    "novelty_thresh = df['novelty_norm'].quantile(0.708)\n",
    "plausibility_thresh = df['plausibility_norm'].quantile(0.708)\n",
    "\n",
    "# Filter rows that exceed both thresholds\n",
    "data = df[(df['novelty_norm'] >= novelty_thresh) & (df['plausibility_norm'] >= plausibility_thresh)]\n",
    "data.head()"
   ],
   "id": "d56d9edbf91e0e3e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = df[(df[\"novelty_norm\"] > 0.8) & (df[\"plausibility_norm\"] > 0.1)]\n",
    "data = data.drop(columns=['novelty_norm', 'plausibility_norm', 'Unnamed: 0', 'rank'])\n",
    "data['hypothesis_id'].unique().shape"
   ],
   "id": "8cfd0aed32e4520a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data.drop([\"model\"])",
   "id": "7e6cc25cc0267ccf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data_for_model = pd.read_csv(\"\")",
   "id": "5f0bc27e51119119"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data.to_excel(\"llm_results/normalized_new_results.xlsx\", index=False)",
   "id": "27da69187f1af931"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_michal = pd.read_excel(\"llm_results/michal_input.xlsx\")\n",
    "# df_michal = df.copy()"
   ],
   "id": "3758c45d345f3fde"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# computing Z score\n",
    "df_michal[['novelty_norm', 'plausibility_norm']] = df_michal[['novelty', 'plausibility']].transform(\n",
    "    lambda x: (x - x.mean()) / x.std()\n",
    ")\n",
    "df_michal"
   ],
   "id": "a90bfd76181ca12c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_small = df[[\"hypothesis_id\", \"model\", \"hypo_factors\", \"novelty_norm\", \"plausibility_norm\"]]\n",
    "df_michal.loc[:, \"model\"] = \"michal\"\n",
    "df_michal = df_michal[[\"hypothesis_id\", \"model\", \"hypo_factors\", \"novelty_norm\", \"plausibility_norm\"]]\n",
    "df_combined = (pd.merge(df_small, df_michal, on='hypothesis_id', how='inner'))\n",
    "df_combined"
   ],
   "id": "5546c67810973009"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_small = df_small[df_small[\"hypothesis_id\"].isin(df_michal.hypothesis_id)]",
   "id": "230908c1a38b9ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_concat = pd.concat([df_small, df_michal], ignore_index=True)\n",
    "df_concat"
   ],
   "id": "b1df68a7e54595df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculate correlation\n",
    "correlation = df_combined['novelty_norm_x'].corr(df_combined['novelty_norm_y'])\n",
    "print(f\"Correlation - Novelty: {correlation:.2f}\")\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(df_combined['novelty_norm_x'], df_combined['novelty_norm_y'])\n",
    "print(f\"\\nMean Absolute Error - Novelty: {mae:.2f}\")\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = df_combined['plausibility_norm_x'].corr(df_combined['plausibility_norm_y'])\n",
    "print(f\"\\nCorrelation - plausibility: {correlation:.2f}\")\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(df_combined['plausibility_norm_x'], df_combined['plausibility_norm_y'])\n",
    "print(f\"\\nMean Absolute Error - plausibility: {mae:.2f}\")\n",
    "\n"
   ],
   "id": "c30e4a4490fd0634"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_pivot = df_concat.pivot(index='hypothesis_id', columns='model', values='novelty_norm')\n",
    "\n",
    "# Compute standard deviation per sample (variation between models)\n",
    "df_pivot['std'] = df_pivot.std(axis=1)\n",
    "df_pivot['range'] = df_pivot.max(axis=1) - df_pivot.min(axis=1)\n",
    "\n",
    "# Compute pairwise MAE and RMSE between models\n",
    "def compare_models(col1, col2):\n",
    "    mae = mean_absolute_error(df_pivot[col1], df_pivot[col2])\n",
    "    rmse = np.sqrt(mean_squared_error(df_pivot[col1], df_pivot[col2]))\n",
    "    return mae, rmse\n",
    "\n",
    "mae_ab, rmse_ab = compare_models('OpenAI o4-mini', 'Claude 3.7 Sonnet')\n",
    "mae_ac, rmse_ac = compare_models('OpenAI o4-mini', 'OpenAI o3-mini')\n",
    "mae_bc, rmse_bc = compare_models('Claude 3.7 Sonnet', 'OpenAI o3-mini')\n",
    "mae_db, rmse_db = compare_models('michal', 'Claude 3.7 Sonnet')\n",
    "mae_dc, rmse_dc = compare_models('michal', 'OpenAI o3-mini')\n",
    "mae_da, rmse_da = compare_models('michal', 'OpenAI o4-mini')\n",
    "\n",
    "# Output results\n",
    "\n",
    "print(\"Mean Absolute Errors - novelty:\")\n",
    "print(f\"Model o4-mini vs anthropic: {mae_ab:.2f}\")\n",
    "print(f\"Model o4-mini vs o3-mini: {mae_ac:.2f}\")\n",
    "print(f\"Model anthropic vs o3-mini: {mae_bc:.2f}\")\n",
    "print(f\"Model michal vs anthropic: {mae_ab:.2f}\")\n",
    "print(f\"Model michal vs o3-mini: {mae_ac:.2f}\")\n",
    "print(f\"Model michal vs o4-mini: {mae_bc:.2f}\")\n",
    "\n",
    "print(\"\\nRMSE - novelty:\")\n",
    "print(f\"Model o4-mini vs anthropic: {rmse_ab:.2f}\")\n",
    "print(f\"Model o4-mini vs o3-mini: {rmse_ac:.2f}\")\n",
    "print(f\"Model anthropic vs o3-mini: {rmse_bc:.2f}\")\n",
    "print(f\"Model michal vs anthropic: {rmse_ab:.2f}\")\n",
    "print(f\"Model michal vs o3-mini: {rmse_ac:.2f}\")\n",
    "print(f\"Model michal vs o4-mini: {rmse_bc:.2f}\")\n",
    "\n",
    "df_pivot = df_concat.pivot(index='hypothesis_id', columns='model', values='plausibility_norm')\n",
    "\n",
    "mae_ab, rmse_ab = compare_models('OpenAI o4-mini', 'Claude 3.7 Sonnet')\n",
    "mae_ac, rmse_ac = compare_models('OpenAI o4-mini', 'OpenAI o3-mini')\n",
    "mae_bc, rmse_bc = compare_models('Claude 3.7 Sonnet', 'OpenAI o3-mini')\n",
    "mae_db, rmse_db = compare_models('michal', 'Claude 3.7 Sonnet')\n",
    "mae_dc, rmse_dc = compare_models('michal', 'OpenAI o3-mini')\n",
    "mae_da, rmse_da = compare_models('michal', 'OpenAI o4-mini')\n",
    "\n",
    "# Output results\n",
    "\n",
    "print(\"\\nMean Absolute Errors - plausibility:\")\n",
    "print(f\"Model o4-mini vs anthropic: {mae_ab:.2f}\")\n",
    "print(f\"Model o4-mini vs o3-mini: {mae_ac:.2f}\")\n",
    "print(f\"Model anthropic vs o3-mini: {mae_bc:.2f}\")\n",
    "print(f\"Model michal vs anthropic: {mae_ab:.2f}\")\n",
    "print(f\"Model michal vs o3-mini: {mae_ac:.2f}\")\n",
    "print(f\"Model michal vs o4-mini: {mae_bc:.2f}\")\n",
    "\n",
    "print(\"\\nRMSE - plausibility:\")\n",
    "print(f\"Model o4-mini vs anthropic: {rmse_ab:.2f}\")\n",
    "print(f\"Model o4-mini vs o3-mini: {rmse_ac:.2f}\")\n",
    "print(f\"Model anthropic vs o3-mini: {rmse_bc:.2f}\")\n",
    "print(f\"Model michal vs anthropic: {rmse_ab:.2f}\")\n",
    "print(f\"Model michal vs o3-mini: {rmse_ac:.2f}\")\n",
    "print(f\"Model michal vs o4-mini: {rmse_bc:.2f}\")"
   ],
   "id": "e5a70c7462d00351"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define model order for consistent axes\n",
    "model_order = ['OpenAI o4-mini', 'Claude 3.7 Sonnet', 'OpenAI o3-mini', 'michal']\n",
    "\n",
    "# Function to compute pairwise metric matrix\n",
    "def pairwise_metric(df_pivot, metric_func, models):\n",
    "    n = len(models)\n",
    "    matrix = np.zeros((n, n))\n",
    "    for i, m1 in enumerate(models):\n",
    "        for j, m2 in enumerate(models):\n",
    "            if i != j:\n",
    "                matrix[i, j] = metric_func(df_pivot[m1], df_pivot[m2])\n",
    "            else:\n",
    "                matrix[i, j] = np.nan\n",
    "    return matrix\n",
    "\n",
    "# Compute MAE and RMSE matrices for novelty\n",
    "df_nov = df_concat.pivot(index='hypothesis_id', columns='model', values='novelty_norm')[model_order]\n",
    "mae_matrix_nov = pairwise_metric(df_nov, mean_absolute_error, model_order)\n",
    "rmse_matrix_nov = pairwise_metric(df_nov, lambda x, y: np.sqrt(mean_squared_error(x, y)), model_order)\n",
    "\n",
    "# Compute MAE and RMSE matrices for plausibility\n",
    "df_plau = df_concat.pivot(index='hypothesis_id', columns='model', values='plausibility_norm')[model_order]\n",
    "mae_matrix_plau = pairwise_metric(df_plau, mean_absolute_error, model_order)\n",
    "rmse_matrix_plau = pairwise_metric(df_plau, lambda x, y: np.sqrt(mean_squared_error(x, y)), model_order)\n",
    "\n",
    "# Plot heatmaps\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "sns.heatmap(mae_matrix_nov, annot=True, fmt=\".2f\", xticklabels=model_order, yticklabels=model_order, ax=axes[0,0], cmap=\"Blues\")\n",
    "axes[0,0].set_title(\"MAE - Novelty\")\n",
    "sns.heatmap(rmse_matrix_nov, annot=True, fmt=\".2f\", xticklabels=model_order, yticklabels=model_order, ax=axes[0,1], cmap=\"Greens\")\n",
    "axes[0,1].set_title(\"RMSE - Novelty\")\n",
    "sns.heatmap(mae_matrix_plau, annot=True, fmt=\".2f\", xticklabels=model_order, yticklabels=model_order, ax=axes[1,0], cmap=\"Blues\")\n",
    "axes[1,0].set_title(\"MAE - Plausibility\")\n",
    "sns.heatmap(rmse_matrix_plau, annot=True, fmt=\".2f\", xticklabels=model_order, yticklabels=model_order, ax=axes[1,1], cmap=\"Greens\")\n",
    "axes[1,1].set_title(\"RMSE - Plausibility\")\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlabel(\"Model\")\n",
    "    ax.set_ylabel(\"Model\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "a29f178817ae9b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "full_data = pd.read_csv(\"narrowed_cancers_data.csv\")\n",
    "# full_data = full_data.drop('_tmpkey', axis=1)"
   ],
   "id": "c84a09f4c3e77286"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Mapping from df_michal to full_data column names\n",
    "rename_dict = {\n",
    "    'cancer_type': 'Cancer Type',\n",
    "    'Diagnosis Age': 'Diagnosis Age',\n",
    "    'Smoke Status': 'Smoke Status',\n",
    "    'TMB (nonsynonymous)': 'TMB (nonsynonymous)',\n",
    "    'Hugo_Symbol': 'Hugo_Symbol',\n",
    "    'Chromosome': 'Chromosome',\n",
    "    'Start_Position': 'Start_Position',\n",
    "    'End_Position': 'End_Position',\n",
    "    'Variant_Type': 'Variant_Type',\n",
    "    'SNP_event': 'SNP_event',\n",
    "    'Protein_position': 'Protein_position',\n",
    "    'Codons': 'Codons',\n",
    "    'Exon_Number': 'Exon_Number',\n",
    "    'VAR_TYPE_SX': 'VAR_TYPE_SX',\n",
    "    'Site1_Hugo_Symbol': 'Site1_Hugo_Symbol',\n",
    "    'Site2_Hugo_Symbol': 'Site2_Hugo_Symbol',\n",
    "    'Event_Info': 'Event_Info',\n",
    "    'missense_variant': 'missense_variant',\n",
    "    'Sex': 'Sex',\n",
    "    'splice_acceptor_variant': 'splice_acceptor_variant',\n",
    "    'upstream_gene_variant': 'upstream_gene_variant'\n",
    "    # Add more mappings if needed\n",
    "}\n",
    "\n",
    "# Rename columns in df_michal\n",
    "data = df.copy()\n",
    "data_renamed = data.rename(columns=rename_dict)\n",
    "data_renamed"
   ],
   "id": "d79659dce457f2a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data_renamed.columns",
   "id": "4b96bf836c078f19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "full_data.columns",
   "id": "13feb97aee3b4eca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data_renamed['Start_Position'] = data_renamed['Start_Position'].astype(str)\n",
    "full_data['Start_Position'] = full_data['Start_Position'].astype(str)\n",
    "data_renamed['End_Position'] = data_renamed['End_Position'].astype(str)\n",
    "full_data['End_Position'] = full_data['End_Position'].astype(str)\n",
    "data_renamed['upstream_gene_variant'] = data_renamed['upstream_gene_variant'].astype(str)\n",
    "full_data['upstream_gene_variant'] = full_data['upstream_gene_variant'].astype(str)\n",
    "data_renamed['TMB (nonsynonymous)'] = data_renamed['TMB (nonsynonymous)'].astype(str)\n",
    "full_data['TMB (nonsynonymous)'] = full_data['TMB (nonsynonymous)'].astype(str)\n",
    "data_renamed['missense_variant'] = data_renamed['missense_variant'].astype(str)\n",
    "full_data['missense_variant'] = full_data['missense_variant'].astype(str)\n",
    "data_renamed['Protein_position'] = data_renamed['Protein_position'].astype(str)\n",
    "full_data['Protein_position'] = full_data['Protein_position'].astype(str)\n",
    "data_renamed['splice_acceptor_variant'] = data_renamed['splice_acceptor_variant'].astype(str)\n",
    "full_data['splice_acceptor_variant'] = full_data['splice_acceptor_variant'].astype(str)"
   ],
   "id": "48644d7d4f3962b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data_renamed.columns",
   "id": "4e02b47d75a9042a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def find_matching_patients(df_transformed, data_for_lift):\n",
    "    df_transformed = df_transformed.copy()\n",
    "    data_for_lift = data_for_lift.copy()\n",
    "    cancer_type_groups = data_for_lift.groupby('Cancer Type')\n",
    "    matched_ids = []\n",
    "    matched_counts = []\n",
    "    total_matched_feature_counts = []\n",
    "\n",
    "    for idx, row in df_transformed.iterrows():\n",
    "        cancer_type = row['Cancer Type']\n",
    "        filter_columns = row.drop(labels=['Cancer Type', 'hypothesis_id', 'model', 'novelty', 'plausibility', 'std_novelty', 'std_plausibility', 'hypo_factors', 'support', 'method', 'Unnamed: 0', 'Position', 'novelty_norm', 'rank', 'plausibility_norm']).dropna()\n",
    "\n",
    "        # Find matched patient IDs for this cancer type\n",
    "        if cancer_type not in cancer_type_groups.groups:\n",
    "            matched = []\n",
    "        else:\n",
    "            subset = cancer_type_groups.get_group(cancer_type)\n",
    "            if filter_columns.empty:\n",
    "                matched = subset['PATIENT_ID'].dropna().unique()\n",
    "            else:\n",
    "                mask = pd.Series(True, index=subset.index)\n",
    "                for col, val in filter_columns.items():\n",
    "                    if col in subset.columns:\n",
    "                        mask &= subset[col] == val\n",
    "                matched = subset.loc[mask, 'PATIENT_ID'].dropna().unique()\n",
    "        matched_ids.append(','.join(map(str, matched)))\n",
    "        matched_counts.append(len(matched))\n",
    "\n",
    "        # Find total matched patient IDs with these features across all cancer types\n",
    "        if filter_columns.empty:\n",
    "            total_matched = data_for_lift['PATIENT_ID'].dropna().unique()\n",
    "        else:\n",
    "            mask = pd.Series(True, index=data_for_lift.index)\n",
    "            for col, val in filter_columns.items():\n",
    "                if col in data_for_lift.columns:\n",
    "                    mask &= data_for_lift[col] == val\n",
    "            total_matched = data_for_lift.loc[mask, 'PATIENT_ID'].dropna().unique()\n",
    "        total_matched_feature_counts.append(len(total_matched))\n",
    "\n",
    "    df_transformed['Matched PATIENT_IDs'] = matched_ids\n",
    "    df_transformed['Matched_Count'] = matched_counts\n",
    "    df_transformed['Total_Matched_Feature_Count'] = total_matched_feature_counts\n",
    "    return df_transformed"
   ],
   "id": "96f8ff6dad410de1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = find_matching_patients(data_renamed, full_data)\n",
    "data"
   ],
   "id": "e14bce1fe2e44933"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data = data[data[\"Matched_Count\"] >= 15]",
   "id": "bf469539e6f0f51c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "f = full_data[full_data[\"Cancer Type\"] == \"Colorectal Carcinoma\"]\n",
    "f = f[f[\"Hugo_Symbol\"] == \"KRAS\"]\n",
    "f = f[f[\"Protein_position\"] == 12.0]\n",
    "f = f[f[\"SNP_event\"] == \"C>T\"]\n",
    "f[\"PATIENT_ID\"].unique().shape\n",
    "\n",
    "# Protein position value is 12.0 AND Hugo Symbol value is KRAS AND SNP event value is C>T"
   ],
   "id": "15458165cac7f4e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Compute unique PATIENT_ID count per hypothesis_id\n",
    "patient_counts = result.groupby('hypothesis_id')['PATIENT_ID'].nunique().reset_index()\n",
    "patient_counts.rename(columns={'PATIENT_ID': 'unique_patient_count'}, inplace=True)\n",
    "\n",
    "# Merge into df_michal_renamed\n",
    "df_michal_renamed = df_michal_renamed.merge(patient_counts, on='hypothesis_id', how='left')"
   ],
   "id": "9997715c0abe363"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_michal_renamed.sort_values(by=[\"novelty\", 'unique_patient_count', \"plausibility\"], ascending=[False, False, False], inplace=True)\n",
    "df_michal_renamed"
   ],
   "id": "137ae7b32b8f76d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dd = df_michal_renamed[df_michal_renamed[\"unique_patient_count\"] > 3]\n",
    "dd"
   ],
   "id": "3b20de7513962e25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_michal_renamed[df_michal_renamed[\"hypothesis_id\"].str.startswith(\"LIFT\")]",
   "id": "21f3c6309259de4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_michal = pd.read_excel(\"llm_results/michal_input.xlsx\")\n",
    "merged = data.merge(df_michal, on='hypothesis_id', how='left')\n",
    "merged[[\"hypothesis_id\", \"model\", \"hypo_factors_x\", \"cancer_type\",\"novelty_x\", \"plausibility_x\", \"novelty_y\", \"plausibility_y\", \"unique_patient_count\", \"comments\"]].drop_duplicates()"
   ],
   "id": "13fbfe47e092ad5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "results_lift = pd.read_csv(\"llm_results/evaluations_lift_20250528_144638.csv\")",
   "id": "d9f57462cd190a91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "combine_hyp_lift = pd.read_csv('models_hypotheses/combined_hypotheses_lift_only.csv')\n",
    "combine_hyp_lift.rename(columns={'hypo_id': 'hypothesis_id'}, inplace=True)\n",
    "results_lift = results_lift.join(combine_hyp_lift.set_index('hypothesis_id'), on='hypothesis_id')"
   ],
   "id": "74b1656369942c52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df[\"plausibility\"].round().value_counts()",
   "id": "4377eb9028ab9235"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df[\"novelty\"].round().value_counts()",
   "id": "9e821c343183496"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create a pivot table of counts\n",
    "pivot = data.groupby([data['novelty_norm'].round(), data['plausibility_norm'].round()]).size().unstack(fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(pivot, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
    "plt.xlabel(\"Plausibility\")\n",
    "plt.ylabel(\"Novelty\")\n",
    "plt.title(\"Count of (Novelty, Plausibility) Pairs\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "fbf6b2a316899a06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data = data[~data[\"hypothesis_id\"].str.startswith(\"LIFT\")]",
   "id": "cdcea0cafabc3374"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data2 = data[[\"hypothesis_id\", \"novelty_norm\", \"plausibility_norm\", \"Cancer Type\", \"hypo_factors\", \"Matched_Count\"]].drop_duplicates()\n",
    "dd = data2[(data2[\"novelty_norm\"] >= 0.5) & (data2[\"plausibility_norm\"] >= -0.2)].copy()\n",
    "dd.rename(columns={'Matched_Count': 'support', 'Cancer Type': 'cancer_type'}, inplace=True)\n",
    "dd = dd.drop_duplicates(subset=['hypothesis_id'], keep='first')\n",
    "dd  # [\"hypothesis_id\"].unique().shape"
   ],
   "id": "ffabeef7c8f50291"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_max_from_str(val):\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    vals = [float(x) for x in str(val).split(',') if x.strip()]\n",
    "    return max(vals) if vals else None\n",
    "\n",
    "df['support'] = df['support'].apply(get_max_from_str)"
   ],
   "id": "98d6ea0f6915bca6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df2 = df[[\"hypothesis_id\", \"novelty_norm\", \"plausibility_norm\", \"cancer_type\", \"hypo_factors\", \"max_lift\", \"support\"]].copy()\n",
    "df2.drop_duplicates(inplace=True)\n",
    "d = df2[(df2[\"novelty_norm\"] > 0.6) & (df2[\"plausibility_norm\"] > 0)]\n",
    "# d['support'] = d['support'].astype(float).astype(int)\n",
    "d[\"hypothesis_id\"].unique().shape\n",
    "d = d.drop_duplicates(subset=['hypothesis_id'], keep='first')\n",
    "d"
   ],
   "id": "aa26485fa9bf179c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "case_stady = d.merge(dd, on=['hypothesis_id', 'novelty_norm', 'plausibility_norm', 'cancer_type', 'hypo_factors', 'support'], how='outer')",
   "id": "cbe81eb71c3693dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "case_stady.to_excel(\"llm_results/case_study_candidates.xlsx\", index=False)",
   "id": "e9cfe822ba1411a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "results_lift[results_lift[\"hypothesis_id\"] == \"LIFT.GASTRIC_CANCER.901\"]",
   "id": "e5682ee1ca79ea96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.regplot(\n",
    "    data=results_lift,\n",
    "    x='novelty',\n",
    "    y='max_lift',\n",
    "    scatter_kws={'alpha':0.6},\n",
    "    line_kws={'color':'red'}\n",
    ")\n",
    "plt.title(\"Correlation between Plausibility and Max Lift\")\n",
    "plt.xlabel(\"Plausibility\")\n",
    "plt.ylabel(\"Max Lift\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "c0087b0f1769e8f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df[[\"hypo_factors\", \"cancer_type\"]]",
   "id": "de915220c0052aa5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "64db4511a51e5355"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
